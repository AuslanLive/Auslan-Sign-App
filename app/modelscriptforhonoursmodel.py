# -*- coding: utf-8 -*-
"""rusttesty.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12XfT3FFMPCOvM11x285ZSTyC7TCe60s7
"""

# If running on Colab, mount Drive
from google.colab import drive
drive.mount('/content/drive')

# ==== CHANGE THIS ONLY IF YOUR DRIVE PATH IS DIFFERENT ====
KP_ROOT  ="/content/drive/MyDrive/109-keypoints-all-64"
JSON_DIR = f"{KP_ROOT}/Cutdown-json"

# Manifests produced by your extractor (note: *_150_kp.json)
TRAIN_MAN = f"{JSON_DIR}/Train_150_kp.json"
VAL_MAN   = f"{JSON_DIR}/Valid_150_kp.json"
TEST_MANS = {
    "ITW": f"{JSON_DIR}/Test_ITW_150_kp.json",
    "STU": f"{JSON_DIR}/Test_STU_150_kp.json",
    "SYN": f"{JSON_DIR}/Test_SYN_150_kp.json",
    "TED": f"{JSON_DIR}/Test_TED_150_kp.json",
}

# Where to save model/callbacks (inside 109-keypoints-all)
CALLBACK_DIR = f"{KP_ROOT}/model-callbacks-keras"
MODEL_PATH   = f"{CALLBACK_DIR}/bilstm_best.keras"

import os, json, numpy as np, tensorflow as tf
from pathlib import Path
os.makedirs(CALLBACK_DIR, exist_ok=True)

print("KP_ROOT:", KP_ROOT)
print("JSON_DIR:", JSON_DIR)

import json, numpy as np, os
from pathlib import Path

LABEL_MAP_JSON = f"{KP_ROOT}/label_map.json"
STATS_JSON     = f"{KP_ROOT}/stats.json"

def build_label_map(train_manifest, out_json):
    with open(train_manifest, "r", encoding="utf-8") as f:
        d = json.load(f)
    glosses = sorted({v["gloss"] for _, v in d.items()})
    lm = {g:i for i,g in enumerate(glosses)}
    with open(out_json, "w", encoding="utf-8") as f:
        json.dump(lm, f, indent=2, ensure_ascii=False)
    print(f"Saved label_map to {out_json} with {len(lm)} classes.")
    return out_json

def compute_stats(train_manifest, kp_root, out_json, max_items=None):
    with open(train_manifest, "r", encoding="utf-8") as f:
        items = list(json.load(f).items())

    feats = []
    take = items if max_items is None else items[:max_items]
    for _, meta in take:
        p = Path(kp_root) / meta["npz"]
        d = np.load(p)
        x = d["x"].astype(np.float32).reshape(d["x"].shape[0], -1)  # [T,84]
        m = d["m"].astype(np.float32)                               # [T,42]
        m_feat = np.repeat(m, 2, axis=1)                            # [T,84]
        x = x * m_feat
        feats.append(x)

    X = np.concatenate(feats, axis=0)  # [N*T,84]
    mu = X.mean(axis=0).tolist()
    sd = X.std(axis=0).tolist()
    with open(out_json, "w") as f:
        json.dump({"mean": mu, "std": sd}, f)
    print(f"Saved stats to {out_json}")
    return out_json

if not os.path.exists(LABEL_MAP_JSON):
    build_label_map(TRAIN_MAN, LABEL_MAP_JSON)
else:
    print("label_map.json already exists")

if not os.path.exists(STATS_JSON):
    compute_stats(TRAIN_MAN, KP_ROOT, STATS_JSON)   # set max_items if you want a quick pass
else:
    print("stats.json already exists")

# quick peek
with open(LABEL_MAP_JSON, "r") as f:
    lm = json.load(f)
print("Classes:", len(lm))

import tensorflow as tf
import numpy as np
import json, os
from pathlib import Path, PureWindowsPath

# Load maps/stats
with open(LABEL_MAP_JSON, "r", encoding="utf-8") as f:
    label_map = json.load(f)
inv_label = {v:k for k,v in label_map.items()}

with open(STATS_JSON, "r") as f:
    stats = json.load(f)
mu = np.array(stats["mean"], dtype=np.float32)
sd = np.array(stats["std"],  dtype=np.float32)
sd[sd < 1e-6] = 1.0

T = 64
F = 84

def resolve_npz(npz_rel):
    # robust join (handles Windows-like separators stored in JSON)
    rel = Path(PureWindowsPath(npz_rel))
    return str(Path(KP_ROOT) / rel)

def load_index(man_path):
    with open(man_path, "r", encoding="utf-8") as f:
        d = json.load(f)
    items = []
    for _, meta in d.items():
        items.append((meta["npz"], label_map[str(meta["gloss"])]))
    return items

def _decode_tf_string(s):
    if hasattr(s, "numpy"):
        return s.numpy().decode("utf-8")
    if isinstance(s, (bytes, bytearray, np.bytes_)):
        return bytes(s).decode("utf-8")
    if isinstance(s, str):
        return s
    return str(s)

def npz_to_example(npz_rel, y):
    p = resolve_npz(_decode_tf_string(npz_rel))
    d = np.load(p)
    x = d["x"].astype(np.float32)       # [T,42,2]
    m = d["m"].astype(np.float32)       # [T,42]
    x = x.reshape(x.shape[0], -1)       # [T,84]
    m_feat = np.repeat(m, 2, axis=1)    # [T,84]
    x = x * m_feat                      # zero missing
    x = (x - mu) / sd                   # z-score
    return x, np.int32(y)

def tf_load(npz_rel, y):
    x, y = tf.py_function(
        func=npz_to_example,
        inp=[npz_rel, y],
        Tout=[tf.float32, tf.int32]
    )
    x.set_shape([T, F])
    y.set_shape([])
    return x, y

def make_dataset(man_path, bsz=128, shuffle=True):
    items = load_index(man_path)
    paths = tf.constant([p for p,_ in items], dtype=tf.string)
    ys    = tf.constant([y for _,y in items], dtype=tf.int32)
    ds = tf.data.Dataset.from_tensor_slices((paths, ys))
    if shuffle:
        ds = ds.shuffle(buffer_size=len(items), reshuffle_each_iteration=True)
    ds = ds.map(tf_load, num_parallel_calls=tf.data.AUTOTUNE)
    ds = ds.batch(bsz).prefetch(tf.data.AUTOTUNE)
    return ds, len(items)

BATCH = 128
train_ds, Ntrain = make_dataset(TRAIN_MAN, bsz=BATCH, shuffle=True)
val_ds,   Nval   = make_dataset(VAL_MAN,   bsz=BATCH, shuffle=False)
print("Ntrain:", Ntrain, "Nval:", Nval, "| Classes:", len(label_map))

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

NUM_CLASSES = len(label_map)

def build_model(T=64, F=84, C=NUM_CLASSES):
    inp = keras.Input(shape=(T, F), name="seq")
    x = layers.Masking(mask_value=0.0)(inp)

    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(x)
    x = layers.Dropout(0.3)(x)
    x = layers.Bidirectional(layers.LSTM(256, return_sequences=False))(x)
    x = layers.Dropout(0.3)(x)

    x = layers.Dense(256, activation="relu")(x)
    x = layers.Dropout(0.3)(x)
    out = layers.Dense(C, activation="softmax")(x)

    model = keras.Model(inp, out, name="bilstm_kp")
    return model

model = build_model(T, F, NUM_CLASSES)
model.summary()

model.compile(
    optimizer=keras.optimizers.Adam(1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger

ckpt = ModelCheckpoint(
    MODEL_PATH,
    monitor="val_accuracy",
    mode="max",
    save_best_only=True,
    save_weights_only=False,   # Keras v3 SavedModel/keras format
    verbose=1
)
lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3, verbose=1)
es = EarlyStopping(monitor="val_loss", patience=8, restore_best_weights=True, verbose=1)
log = CSVLogger(f"{CALLBACK_DIR}/train_log.csv")

callbacks = [ckpt, lr, es, log]
print("Callbacks will save to:", CALLBACK_DIR)

EPOCHS = 60
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    callbacks=callbacks,
    verbose=1
)
print("Best model saved at:", MODEL_PATH)

# === Smart (re)train cell: never start from scratch ===
import os, csv, tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger

# 1) Build or load model
if os.path.exists(MODEL_PATH):
    print(f"[Resume] Loading best model from:\n  {MODEL_PATH}")
    model = tf.keras.models.load_model(MODEL_PATH, compile=False)
else:
    print("[Fresh] No saved model found â€” building a new one.")
    model = build_model(T, F, NUM_CLASSES)

# (Re)compile (safe even when resuming)
model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

# 2) Where to continue from?
log_path = os.path.join(CALLBACK_DIR, "train_log.csv")
initial_epoch = 0
if os.path.exists(log_path):
    try:
        with open(log_path, "r", newline="") as f:
            n_epochs_logged = sum(1 for _ in csv.DictReader(f))
        initial_epoch = n_epochs_logged
        print(f"[Resume] Found CSV log with {n_epochs_logged} completed epoch(s). "
              f"Continuing from epoch {initial_epoch}.")
    except Exception as e:
        print(f"[Warn] Could not parse CSV log: {e}")

# 3) Callbacks (append to CSV; keep saving best on val_accuracy)
ckpt = ModelCheckpoint(
    MODEL_PATH,
    monitor="val_accuracy",
    mode="max",
    save_best_only=True,
    save_weights_only=False,
    verbose=1,
)
lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3, verbose=1)
es = EarlyStopping(monitor="val_loss", patience=8, restore_best_weights=True, verbose=1)
log = CSVLogger(log_path, append=True)

callbacks = [ckpt, lr, es, log]
print("Callbacks dir:", CALLBACK_DIR)

# 4) Train more (or the first time)
#    If you just want to *skip* training and only evaluate, set DO_TRAIN=False.
DO_TRAIN = True
if DO_TRAIN:
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=EPOCHS,
        initial_epoch=initial_epoch,
        callbacks=callbacks,
        verbose=1,
    )
    print("Best model (so far) saved at:", MODEL_PATH)
else:
    print("Training skipped (DO_TRAIN=False). You can evaluate with the loaded model.")

import numpy as np
from collections import defaultdict

# reload best
best_model = tf.keras.models.load_model(MODEL_PATH)

def topk_acc(y_true, y_prob, k=5):
    topk = np.argpartition(-y_prob, kth=range(k), axis=1)[:, :k]
    return (topk == y_true[:, None]).any(axis=1).mean()

def evaluate_manifest(name, man_path, batch=256):
    ds, N = make_dataset(man_path, bsz=batch, shuffle=False)
    y_true = []
    y_prob = []
    for xb, yb in ds:
        pb = best_model(xb, training=False).numpy()
        y_prob.append(pb)
        y_true.append(yb.numpy())
    y_true = np.concatenate(y_true)
    y_prob = np.concatenate(y_prob)
    y_pred = y_prob.argmax(1)
    acc1 = (y_pred == y_true).mean()
    acc5 = topk_acc(y_true, y_prob, k=5)
    print(f"{name:>4} | top-1 {acc1:.3f} | top-5 {acc5:.3f} | N={len(y_true)}")
    return y_true, y_prob

print("=== Test splits ===")
test_results = {}
for k, p in TEST_MANS.items():
    y_true, y_prob = evaluate_manifest(k, p)
    test_results[k] = (y_true, y_prob)

import numpy as np

def inv(idx): return {v:k for k,v in label_map.items()}[idx]

def show_samples(man_path, n=10):
    ds, _ = make_dataset(man_path, bsz=1, shuffle=False)
    inv_map = {v:k for k,v in label_map.items()}
    shown = 0
    for xb, yb in ds:
        prob = best_model(xb, training=False).numpy()[0]
        top5 = prob.argsort()[-5:][::-1]
        true_idx = int(yb.numpy()[0])
        print(f"[{shown}] true: {inv_map[true_idx]:>12} | pred@5: {[inv_map[i] for i in top5]}")
        shown += 1
        if shown >= n: break

print("Sample predictions from ITW:")
show_samples(TEST_MANS["ITW"], n=10)

!pip -q install scikit-learn
from sklearn.metrics import confusion_matrix
import numpy as np

def confusion_for_split(man_key):
    y_true, y_prob = test_results[man_key]
    y_pred = y_prob.argmax(1)
    cm = confusion_matrix(y_true, y_pred, labels=list(range(NUM_CLASSES)))
    print("Confusion matrix shape:", cm.shape)
    return cm

cm_itw = confusion_for_split("ITW")

# --- Summarize test-set accuracies (top-1/top-5) and save as CSV ---
import numpy as np
import pandas as pd

def evaluate_to_dict(name, man_path):
    ds, _ = make_dataset(man_path, bsz=256, shuffle=False)
    y_true = []
    y_prob = []
    for xb, yb in ds:
        pb = best_model(xb, training=False).numpy()
        y_prob.append(pb)
        y_true.append(yb.numpy())
    y_true = np.concatenate(y_true)
    y_prob = np.concatenate(y_prob)
    y_pred = y_prob.argmax(1)
    top1 = (y_pred == y_true).mean()
    # top-5
    top5_hits = []
    top5_idx = np.argpartition(-y_prob, kth=range(5), axis=1)[:, :5]
    top5 = (top5_idx == y_true[:, None]).any(axis=1).mean()
    return {"split": name, "N": len(y_true), "top1": float(top1), "top5": float(top5)}

rows = []
for split_name, man_path in TEST_MANS.items():
    rows.append(evaluate_to_dict(split_name, man_path))

df = pd.DataFrame(rows).sort_values("split")
display(df.style.format({"top1": "{:.3f}", "top5": "{:.3f}"}))

csv_path = f"{CALLBACK_DIR}/test_summary.csv"
df.to_csv(csv_path, index=False)
print("Saved:", csv_path)

# --- Per-class accuracy for one split (e.g., ITW) ---
split_key = "ITW"  # change to "STU", "SYN", "TED" as needed
ds, _ = make_dataset(TEST_MANS[split_key], bsz=256, shuffle=False)

all_true, all_pred = [], []
for xb, yb in ds:
    pb = best_model(xb, training=False).numpy()
    all_true.append(yb.numpy())
    all_pred.append(pb.argmax(1))

y_true = np.concatenate(all_true)
y_pred = np.concatenate(all_pred)

inv_map = {v:k for k,v in label_map.items()}
classes = sorted(label_map.values())
accs = []
for c in classes:
    mask = (y_true == c)
    if mask.sum() == 0:
        acc = np.nan
    else:
        acc = (y_pred[mask] == c).mean()
    accs.append({"gloss": inv_map[c], "acc": acc, "count": int(mask.sum())})

df_cls = pd.DataFrame(accs).sort_values(["count","acc"], ascending=[False,False])
display(df_cls.head(20).style.format({"acc": "{:.3f}"}))  # top 20 by support

# === Per-class metrics for ALL 109 words across all test splits + pooled ===
import numpy as np
import pandas as pd

# assumes you already have:
# - best_model   (tf.keras.Model, loaded)
# - TEST_MANS    (dict: {"ITW": "...", "STU": "...", "SYN": "...", "TED": "..."})
# - label_map    (dict: gloss -> id)
# - make_dataset (function returning tf.data.Dataset)
# - CALLBACK_DIR (string path)

inv_map = {v: k for k, v in label_map.items()}
num_classes = len(label_map)

def preds_and_labels(manifest_path, bsz=256):
    ds, _ = make_dataset(manifest_path, bsz=bsz, shuffle=False)
    probs, labels = [], []
    for xb, yb in ds:
        pb = best_model(xb, training=False).numpy()
        probs.append(pb)
        labels.append(yb.numpy())
    if not probs:
        return np.zeros((0, num_classes), dtype=np.float32), np.zeros((0,), dtype=np.int64)
    return np.concatenate(probs, 0), np.concatenate(labels, 0)

def per_class_metrics(y_true, y_prob):
    """Return DataFrame with columns: gloss, count, top1, top5."""
    y_pred = y_prob.argmax(1)
    # top-5 membership
    # faster than full sort:
    top5_idx = np.argpartition(-y_prob, kth=4, axis=1)[:, :5]

    rows = []
    for cid in range(num_classes):
        mask = (y_true == cid)
        n = int(mask.sum())
        if n == 0:
            rows.append({"gloss": inv_map[cid], "count": 0, "top1": np.nan, "top5": np.nan})
            continue
        top1 = float((y_pred[mask] == cid).mean())
        top5 = float((top5_idx[mask] == cid).any(axis=1).mean())
        rows.append({"gloss": inv_map[cid], "count": n, "top1": top1, "top5": top5})
    return pd.DataFrame(rows)

# collect per-split frames
split_frames = {}
all_probs = []
all_labels = []

for split_key, man_path in TEST_MANS.items():  # e.g., {"ITW": "...", ...}
    probs, labels = preds_and_labels(man_path, bsz=256)
    all_probs.append(probs)
    all_labels.append(labels)
    df = per_class_metrics(labels, probs)
    df = df.rename(columns={
        "count": f"count_{split_key}",
        "top1":  f"top1_{split_key}",
        "top5":  f"top5_{split_key}",
    })
    split_frames[split_key] = df

# pooled "ALL" across test sets
if all_probs:
    probs_all = np.concatenate(all_probs, 0)
    labels_all = np.concatenate(all_labels, 0)
else:
    probs_all = np.zeros((0, num_classes), dtype=np.float32)
    labels_all = np.zeros((0,), dtype=np.int64)

df_all = per_class_metrics(labels_all, probs_all).rename(columns={
    "count": "count_ALL", "top1": "top1_ALL", "top5": "top5_ALL"
})

# merge everything on gloss
from functools import reduce
dfs_to_merge = [df_all] + [split_frames[k] for k in sorted(split_frames.keys())]
df_merged = reduce(lambda l, r: pd.merge(l, r, on="gloss", how="outer"), dfs_to_merge)

# order columns nicely
col_order = ["gloss",
             "count_ALL", "top1_ALL", "top5_ALL",
             "count_ITW", "top1_ITW", "top5_ITW",
             "count_STU", "top1_STU", "top5_STU",
             "count_SYN", "top1_SYN", "top5_SYN",
             "count_TED", "top1_TED", "top5_TED"]
# keep only existing (in case a split is missing)
col_order = [c for c in col_order if c in df_merged.columns]
df_merged = df_merged[col_order].sort_values("gloss").reset_index(drop=True)

# pretty print & save
pd.set_option("display.max_rows", 1000)
display(df_merged.style.format({c: "{:.3f}" for c in df_merged.columns if c.startswith(("top1","top5"))}))

out_csv = f"{CALLBACK_DIR}/per_class_test_metrics_109.csv"
df_merged.to_csv(out_csv, index=False)
print("Saved per-class test metrics to:", out_csv)

# === Plot training/validation curves (loss & accuracy) ===
import os, csv
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

log_path = Path(CALLBACK_DIR) / "train_log.csv"

# 1) Collect from in-memory histories (present run[s])
histories = []
if "history" in globals() and hasattr(history, "history"):
    histories.append(history.history)
if "history2" in globals() and hasattr(history2, "history"):
    histories.append(history2.history)

def concat_histories(hlist, key):
    vals = []
    for h in hlist:
        if key in h:
            vals.extend(h[key])
    return np.array(vals, dtype=float) if vals else None

h_loss  = concat_histories(histories, "loss")
h_vloss = concat_histories(histories, "val_loss")
h_acc   = concat_histories(histories, "accuracy")
h_vacc  = concat_histories(histories, "val_accuracy")

# 2) Also read the CSV log (spans across runs when append=True)
csv_loss = csv_vloss = csv_acc = csv_vacc = None
if log_path.exists():
    loss_list, vloss_list, acc_list, vacc_list = [], [], [], []
    with open(log_path, "r", newline="") as f:
        reader = csv.DictReader(f)
        for row in reader:
            # Defensive parse (handles missing columns)
            if "loss" in row and row["loss"] != "":
                loss_list.append(float(row["loss"]))
            if "val_loss" in row and row["val_loss"] != "":
                vloss_list.append(float(row["val_loss"]))
            if "accuracy" in row and row["accuracy"] != "":
                acc_list.append(float(row["accuracy"]))
            if "val_accuracy" in row and row["val_accuracy"] != "":
                vacc_list.append(float(row["val_accuracy"]))
    if loss_list:  csv_loss  = np.array(loss_list,  dtype=float)
    if vloss_list: csv_vloss = np.array(vloss_list, dtype=float)
    if acc_list:   csv_acc   = np.array(acc_list,   dtype=float)
    if vacc_list:  csv_vacc  = np.array(vacc_list,  dtype=float)

# === Plot Loss ===
plt.figure(figsize=(7,5))
if h_loss is not None:  plt.plot(h_loss,  label="train loss (this run)")
if h_vloss is not None: plt.plot(h_vloss, label="val loss (this run)")
if csv_loss is not None:  plt.plot(csv_loss,  linestyle="--", label="train loss (CSV)")
if csv_vloss is not None: plt.plot(csv_vloss, linestyle="--", label="val loss (CSV)")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.grid(True)
plt.show()

# === Plot Accuracy ===
plt.figure(figsize=(7,5))
if h_acc is not None:  plt.plot(h_acc,  label="train acc (this run)")
if h_vacc is not None: plt.plot(h_vacc, label="val acc (this run)")
if csv_acc is not None:  plt.plot(csv_acc,  linestyle="--", label="train acc (CSV)")
if csv_vacc is not None: plt.plot(csv_vacc, linestyle="--", label="val acc (CSV)")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Training vs Validation Accuracy")
plt.legend()
plt.grid(True)
plt.show()