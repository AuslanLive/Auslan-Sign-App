# -*- coding: utf-8 -*-
"""Bestof 23rd109wordversionkeypoints.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C0LlSLpx_9CYRggPpob6W8Aqdz8KnN3s
"""

# If running on Colab, mount Drive
from google.colab import drive
drive.mount('/content/drive')

# ==== CHANGE THIS ONLY IF YOUR DRIVE PATH IS DIFFERENT ====
KP_ROOT = "/content/drive/MyDrive/109-keypoints-all"  # your uploaded 109-keypoints-all
JSON_DIR = f"{KP_ROOT}/Cutdown-json"

# Manifests produced by your extractor (note: *_150_kp.json)
TRAIN_MAN = f"{JSON_DIR}/Train_150_kp.json"
VAL_MAN   = f"{JSON_DIR}/Valid_150_kp.json"
TEST_MANS = {
    "ITW": f"{JSON_DIR}/Test_ITW_150_kp.json",
    "STU": f"{JSON_DIR}/Test_STU_150_kp.json",
    "SYN": f"{JSON_DIR}/Test_SYN_150_kp.json",
    "TED": f"{JSON_DIR}/Test_TED_150_kp.json",
}

# Where to save model/callbacks (inside 109-keypoints-all)
CALLBACK_DIR = f"{KP_ROOT}/model-callbacks-keras"
MODEL_PATH   = f"{CALLBACK_DIR}/bilstm_best.keras"

import os, json, numpy as np, tensorflow as tf
from pathlib import Path
os.makedirs(CALLBACK_DIR, exist_ok=True)

print("KP_ROOT:", KP_ROOT)
print("JSON_DIR:", JSON_DIR)

import json, numpy as np, os
from pathlib import Path

LABEL_MAP_JSON = f"{KP_ROOT}/label_map.json"
STATS_JSON     = f"{KP_ROOT}/stats.json"

def build_label_map(train_manifest, out_json):
    with open(train_manifest, "r", encoding="utf-8") as f:
        d = json.load(f)
    glosses = sorted({v["gloss"] for _, v in d.items()})
    lm = {g:i for i,g in enumerate(glosses)}
    with open(out_json, "w", encoding="utf-8") as f:
        json.dump(lm, f, indent=2, ensure_ascii=False)
    print(f"Saved label_map to {out_json} with {len(lm)} classes.")
    return out_json

def compute_stats(train_manifest, kp_root, out_json, max_items=None):
    with open(train_manifest, "r", encoding="utf-8") as f:
        items = list(json.load(f).items())

    feats = []
    take = items if max_items is None else items[:max_items]
    for _, meta in take:
        p = Path(kp_root) / meta["npz"]
        d = np.load(p)
        x = d["x"].astype(np.float32).reshape(d["x"].shape[0], -1)  # [T,84]
        m = d["m"].astype(np.float32)                               # [T,42]
        m_feat = np.repeat(m, 2, axis=1)                            # [T,84]
        x = x * m_feat
        feats.append(x)

    X = np.concatenate(feats, axis=0)  # [N*T,84]
    mu = X.mean(axis=0).tolist()
    sd = X.std(axis=0).tolist()
    with open(out_json, "w") as f:
        json.dump({"mean": mu, "std": sd}, f)
    print(f"Saved stats to {out_json}")
    return out_json

if not os.path.exists(LABEL_MAP_JSON):
    build_label_map(TRAIN_MAN, LABEL_MAP_JSON)
else:
    print("label_map.json already exists")

if not os.path.exists(STATS_JSON):
    compute_stats(TRAIN_MAN, KP_ROOT, STATS_JSON)   # set max_items if you want a quick pass
else:
    print("stats.json already exists")

# quick peek
with open(LABEL_MAP_JSON, "r") as f:
    lm = json.load(f)
print("Classes:", len(lm))

import tensorflow as tf
import numpy as np
import json, os
from pathlib import Path, PureWindowsPath

# Load maps/stats
with open(LABEL_MAP_JSON, "r", encoding="utf-8") as f:
    label_map = json.load(f)
inv_label = {v:k for k,v in label_map.items()}

with open(STATS_JSON, "r") as f:
    stats = json.load(f)
mu = np.array(stats["mean"], dtype=np.float32)
sd = np.array(stats["std"],  dtype=np.float32)
sd[sd < 1e-6] = 1.0

T = 48
F = 84

def resolve_npz(npz_rel):
    # robust join (handles Windows-like separators stored in JSON)
    rel = Path(PureWindowsPath(npz_rel))
    return str(Path(KP_ROOT) / rel)

def load_index(man_path):
    with open(man_path, "r", encoding="utf-8") as f:
        d = json.load(f)
    items = []
    for _, meta in d.items():
        items.append((meta["npz"], label_map[str(meta["gloss"])]))
    return items

def _decode_tf_string(s):
    if hasattr(s, "numpy"):
        return s.numpy().decode("utf-8")
    if isinstance(s, (bytes, bytearray, np.bytes_)):
        return bytes(s).decode("utf-8")
    if isinstance(s, str):
        return s
    return str(s)

def npz_to_example(npz_rel, y):
    p = resolve_npz(_decode_tf_string(npz_rel))
    d = np.load(p)
    x = d["x"].astype(np.float32)       # [T,42,2]
    m = d["m"].astype(np.float32)       # [T,42]
    x = x.reshape(x.shape[0], -1)       # [T,84]
    m_feat = np.repeat(m, 2, axis=1)    # [T,84]
    x = x * m_feat                      # zero missing
    x = (x - mu) / sd                   # z-score
    return x, np.int32(y)

def tf_load(npz_rel, y):
    x, y = tf.py_function(
        func=npz_to_example,
        inp=[npz_rel, y],
        Tout=[tf.float32, tf.int32]
    )
    x.set_shape([T, F])
    y.set_shape([])
    return x, y

def make_dataset(man_path, bsz=128, shuffle=True):
    items = load_index(man_path)
    paths = tf.constant([p for p,_ in items], dtype=tf.string)
    ys    = tf.constant([y for _,y in items], dtype=tf.int32)
    ds = tf.data.Dataset.from_tensor_slices((paths, ys))
    if shuffle:
        ds = ds.shuffle(buffer_size=len(items), reshuffle_each_iteration=True)
    ds = ds.map(tf_load, num_parallel_calls=tf.data.AUTOTUNE)
    ds = ds.batch(bsz).prefetch(tf.data.AUTOTUNE)
    return ds, len(items)

BATCH = 128
train_ds, Ntrain = make_dataset(TRAIN_MAN, bsz=BATCH, shuffle=True)
val_ds,   Nval   = make_dataset(VAL_MAN,   bsz=BATCH, shuffle=False)
print("Ntrain:", Ntrain, "Nval:", Nval, "| Classes:", len(label_map))

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

NUM_CLASSES = len(label_map)

def build_model(T=48, F=84, C=NUM_CLASSES):
    inp = keras.Input(shape=(T, F), name="seq")
    x = layers.Masking(mask_value=0.0)(inp)

    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(x)
    x = layers.Dropout(0.3)(x)
    x = layers.Bidirectional(layers.LSTM(256, return_sequences=False))(x)
    x = layers.Dropout(0.3)(x)

    x = layers.Dense(256, activation="relu")(x)
    x = layers.Dropout(0.3)(x)
    out = layers.Dense(C, activation="softmax")(x)

    model = keras.Model(inp, out, name="bilstm_kp")
    return model

model = build_model(T, F, NUM_CLASSES)
model.summary()

model.compile(
    optimizer=keras.optimizers.Adam(1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger

ckpt = ModelCheckpoint(
    MODEL_PATH,
    monitor="val_accuracy",
    mode="max",
    save_best_only=True,
    save_weights_only=False,   # Keras v3 SavedModel/keras format
    verbose=1
)
lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3, verbose=1)
es = EarlyStopping(monitor="val_loss", patience=8, restore_best_weights=True, verbose=1)
log = CSVLogger(f"{CALLBACK_DIR}/train_log.csv")

callbacks = [ckpt, lr, es, log]
print("Callbacks will save to:", CALLBACK_DIR)

EPOCHS = 60
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    callbacks=callbacks,
    verbose=1
)
print("Best model saved at:", MODEL_PATH)

import numpy as np
from collections import defaultdict

# reload best
best_model = tf.keras.models.load_model(MODEL_PATH)

def topk_acc(y_true, y_prob, k=5):
    topk = np.argpartition(-y_prob, kth=range(k), axis=1)[:, :k]
    return (topk == y_true[:, None]).any(axis=1).mean()

def evaluate_manifest(name, man_path, batch=256):
    ds, N = make_dataset(man_path, bsz=batch, shuffle=False)
    y_true = []
    y_prob = []
    for xb, yb in ds:
        pb = best_model(xb, training=False).numpy()
        y_prob.append(pb)
        y_true.append(yb.numpy())
    y_true = np.concatenate(y_true)
    y_prob = np.concatenate(y_prob)
    y_pred = y_prob.argmax(1)
    acc1 = (y_pred == y_true).mean()
    acc5 = topk_acc(y_true, y_prob, k=5)
    print(f"{name:>4} | top-1 {acc1:.3f} | top-5 {acc5:.3f} | N={len(y_true)}")
    return y_true, y_prob

print("=== Test splits ===")
test_results = {}
for k, p in TEST_MANS.items():
    y_true, y_prob = evaluate_manifest(k, p)
    test_results[k] = (y_true, y_prob)

import numpy as np

def inv(idx): return {v:k for k,v in label_map.items()}[idx]

def show_samples(man_path, n=10):
    ds, _ = make_dataset(man_path, bsz=1, shuffle=False)
    inv_map = {v:k for k,v in label_map.items()}
    shown = 0
    for xb, yb in ds:
        prob = best_model(xb, training=False).numpy()[0]
        top5 = prob.argsort()[-5:][::-1]
        true_idx = int(yb.numpy()[0])
        print(f"[{shown}] true: {inv_map[true_idx]:>12} | pred@5: {[inv_map[i] for i in top5]}")
        shown += 1
        if shown >= n: break

print("Sample predictions from ITW:")
show_samples(TEST_MANS["ITW"], n=10)

!pip -q install scikit-learn
from sklearn.metrics import confusion_matrix
import numpy as np

def confusion_for_split(man_key):
    y_true, y_prob = test_results[man_key]
    y_pred = y_prob.argmax(1)
    cm = confusion_matrix(y_true, y_pred, labels=list(range(NUM_CLASSES)))
    print("Confusion matrix shape:", cm.shape)
    return cm

cm_itw = confusion_for_split("ITW")

# --- Summarize test-set accuracies (top-1/top-5) and save as CSV ---
import numpy as np
import pandas as pd

def evaluate_to_dict(name, man_path):
    ds, _ = make_dataset(man_path, bsz=256, shuffle=False)
    y_true = []
    y_prob = []
    for xb, yb in ds:
        pb = best_model(xb, training=False).numpy()
        y_prob.append(pb)
        y_true.append(yb.numpy())
    y_true = np.concatenate(y_true)
    y_prob = np.concatenate(y_prob)
    y_pred = y_prob.argmax(1)
    top1 = (y_pred == y_true).mean()
    # top-5
    top5_hits = []
    top5_idx = np.argpartition(-y_prob, kth=range(5), axis=1)[:, :5]
    top5 = (top5_idx == y_true[:, None]).any(axis=1).mean()
    return {"split": name, "N": len(y_true), "top1": float(top1), "top5": float(top5)}

rows = []
for split_name, man_path in TEST_MANS.items():
    rows.append(evaluate_to_dict(split_name, man_path))

df = pd.DataFrame(rows).sort_values("split")
display(df.style.format({"top1": "{:.3f}", "top5": "{:.3f}"}))

csv_path = f"{CALLBACK_DIR}/test_summary.csv"
df.to_csv(csv_path, index=False)
print("Saved:", csv_path)

# --- Per-class accuracy for one split (e.g., ITW) ---
split_key = "ITW"  # change to "STU", "SYN", "TED" as needed
ds, _ = make_dataset(TEST_MANS[split_key], bsz=256, shuffle=False)

all_true, all_pred = [], []
for xb, yb in ds:
    pb = best_model(xb, training=False).numpy()
    all_true.append(yb.numpy())
    all_pred.append(pb.argmax(1))

y_true = np.concatenate(all_true)
y_pred = np.concatenate(all_pred)

inv_map = {v:k for k,v in label_map.items()}
classes = sorted(label_map.values())
accs = []
for c in classes:
    mask = (y_true == c)
    if mask.sum() == 0:
        acc = np.nan
    else:
        acc = (y_pred[mask] == c).mean()
    accs.append({"gloss": inv_map[c], "acc": acc, "count": int(mask.sum())})

df_cls = pd.DataFrame(accs).sort_values(["count","acc"], ascending=[False,False])
display(df_cls.head(20).style.format({"acc": "{:.3f}"}))  # top 20 by support

# === Per-class metrics for ALL 109 words across all test splits + pooled ===
import numpy as np
import pandas as pd

# assumes you already have:
# - best_model   (tf.keras.Model, loaded)
# - TEST_MANS    (dict: {"ITW": "...", "STU": "...", "SYN": "...", "TED": "..."})
# - label_map    (dict: gloss -> id)
# - make_dataset (function returning tf.data.Dataset)
# - CALLBACK_DIR (string path)

inv_map = {v: k for k, v in label_map.items()}
num_classes = len(label_map)

def preds_and_labels(manifest_path, bsz=256):
    ds, _ = make_dataset(manifest_path, bsz=bsz, shuffle=False)
    probs, labels = [], []
    for xb, yb in ds:
        pb = best_model(xb, training=False).numpy()
        probs.append(pb)
        labels.append(yb.numpy())
    if not probs:
        return np.zeros((0, num_classes), dtype=np.float32), np.zeros((0,), dtype=np.int64)
    return np.concatenate(probs, 0), np.concatenate(labels, 0)

def per_class_metrics(y_true, y_prob):
    """Return DataFrame with columns: gloss, count, top1, top5."""
    y_pred = y_prob.argmax(1)
    # top-5 membership
    # faster than full sort:
    top5_idx = np.argpartition(-y_prob, kth=4, axis=1)[:, :5]

    rows = []
    for cid in range(num_classes):
        mask = (y_true == cid)
        n = int(mask.sum())
        if n == 0:
            rows.append({"gloss": inv_map[cid], "count": 0, "top1": np.nan, "top5": np.nan})
            continue
        top1 = float((y_pred[mask] == cid).mean())
        top5 = float((top5_idx[mask] == cid).any(axis=1).mean())
        rows.append({"gloss": inv_map[cid], "count": n, "top1": top1, "top5": top5})
    return pd.DataFrame(rows)

# collect per-split frames
split_frames = {}
all_probs = []
all_labels = []

for split_key, man_path in TEST_MANS.items():  # e.g., {"ITW": "...", ...}
    probs, labels = preds_and_labels(man_path, bsz=256)
    all_probs.append(probs)
    all_labels.append(labels)
    df = per_class_metrics(labels, probs)
    df = df.rename(columns={
        "count": f"count_{split_key}",
        "top1":  f"top1_{split_key}",
        "top5":  f"top5_{split_key}",
    })
    split_frames[split_key] = df

# pooled "ALL" across test sets
if all_probs:
    probs_all = np.concatenate(all_probs, 0)
    labels_all = np.concatenate(all_labels, 0)
else:
    probs_all = np.zeros((0, num_classes), dtype=np.float32)
    labels_all = np.zeros((0,), dtype=np.int64)

df_all = per_class_metrics(labels_all, probs_all).rename(columns={
    "count": "count_ALL", "top1": "top1_ALL", "top5": "top5_ALL"
})

# merge everything on gloss
from functools import reduce
dfs_to_merge = [df_all] + [split_frames[k] for k in sorted(split_frames.keys())]
df_merged = reduce(lambda l, r: pd.merge(l, r, on="gloss", how="outer"), dfs_to_merge)

# order columns nicely
col_order = ["gloss",
             "count_ALL", "top1_ALL", "top5_ALL",
             "count_ITW", "top1_ITW", "top5_ITW",
             "count_STU", "top1_STU", "top5_STU",
             "count_SYN", "top1_SYN", "top5_SYN",
             "count_TED", "top1_TED", "top5_TED"]
# keep only existing (in case a split is missing)
col_order = [c for c in col_order if c in df_merged.columns]
df_merged = df_merged[col_order].sort_values("gloss").reset_index(drop=True)

# pretty print & save
pd.set_option("display.max_rows", 1000)
display(df_merged.style.format({c: "{:.3f}" for c in df_merged.columns if c.startswith(("top1","top5"))}))

out_csv = f"{CALLBACK_DIR}/per_class_test_metrics_109.csv"
df_merged.to_csv(out_csv, index=False)
print("Saved per-class test metrics to:", out_csv)